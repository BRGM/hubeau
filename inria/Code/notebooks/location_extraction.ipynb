{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "sKPy_1S8e8gY"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import json\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import nltk\n",
    "import predict_flair\n",
    "from flair.models import SequenceTagger\n",
    "import fr_core_news_lg as fr\n",
    "import urllib.request\n",
    "from ip2geotools.databases.noncommercial import DbIpCity\n",
    "import re\n",
    "import spacy\n",
    "from SPARQLWrapper import SPARQLWrapper2\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import ipinfo\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from math import cos, asin, sqrt, pi\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bss(query): \n",
    "    \"\"\"\n",
    "     Extract BSS code from query\n",
    "    \"\"\"\n",
    "    # AAAABCDDDD/designation\n",
    "    regex = \"[0-9]{5}[a-zA-Z][0-9]{4}/[a-zA-Z0-9]+\"\n",
    "    match = re.findall(regex, query)\n",
    "    return match\n",
    "    if match :\n",
    "        return re.group(0)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "48caB86KjBOT"
   },
   "outputs": [],
   "source": [
    "def get_insee(location):\n",
    "    \"\"\"\n",
    "    \n",
    "    returns the insee codes of the location + type of location (commune, departement or region) \n",
    "    if type region : return insee code of all departements of the region\n",
    "    returns -1 if location does not exist\n",
    "    \n",
    "    \"\"\"\n",
    "    location_ = location.lower()\n",
    "    print(location_)\n",
    "    url = 'https://geo.api.gouv.fr/communes?nom={c}&fields=nom,code,codesPostaux,'\\\n",
    "              'codeDepartement,codeRegion,population&format=json&geometry=centre'\\\n",
    "              .format(c=location_)\n",
    "    exists = len(json.loads(requests.get(url).text))\n",
    "    similar_communes = []\n",
    "    if exists>0 :\n",
    "        codes = json.loads(requests.get(url).text)\n",
    "        result = [code[\"code\"] for code in codes \\\n",
    "                if  unicodedata.normalize('NFD', code[\"nom\"].lower()).encode('ascii', 'ignore').decode(\"utf-8\") == unicodedata.normalize('NFD', location_).encode('ascii', 'ignore').decode(\"utf-8\")]\n",
    "        \n",
    "        if len(result)>0:\n",
    "            return {\"type\": \"commune\", \"code\": result}\n",
    "        else:\n",
    "            similar_communes = [code[\"code\"] for code in codes]\n",
    "        \n",
    "    url = 'https://geo.api.gouv.fr/departements?nom={c}&fields=nom,code'\\\n",
    "            '&format=json&geometry=centre'\\\n",
    "              .format(c=location_)\n",
    "    exists = len(json.loads(requests.get(url).text))\n",
    "    similar_departements = []\n",
    "    if exists>0 :\n",
    "        codes = json.loads(requests.get(url).text)\n",
    "        result = [code[\"code\"] for code in codes \\\n",
    "                if  unicodedata.normalize('NFD', code[\"nom\"].lower()).encode('ascii', 'ignore').decode(\"utf-8\") == unicodedata.normalize('NFD', location_).encode('ascii', 'ignore').decode(\"utf-8\")]\n",
    "        return result\n",
    "        if len(result)>0:\n",
    "            return {\"type\": \"departement\", \"code\": result}\n",
    "        elif len(similar_communes)==0:\n",
    "            similar_departements = [code[\"code\"] for code in codes]\n",
    "    \n",
    "    url = 'https://geo.api.gouv.fr/regions?nom={c}&fields=nom,code'\\\n",
    "            '&format=json&geometry=centre'\\\n",
    "              .format(c=location_)\n",
    "    exists = len(json.loads(requests.get(url).text))\n",
    "    similar_regions = []\n",
    "    if exists>0 :\n",
    "        codes = json.loads(requests.get(url).text)\n",
    "        result = [code[\"code\"] for code in codes \\\n",
    "                if  unicodedata.normalize('NFD', code[\"nom\"].lower()).encode('ascii', 'ignore').decode(\"utf-8\") == unicodedata.normalize('NFD', location_).encode('ascii', 'ignore').decode(\"utf-8\")]\n",
    "        \n",
    "        \n",
    "        if len(result)>0:         \n",
    "            url = \"https://geo.api.gouv.fr/regions/{c}/departements\".format(c=result[0])\n",
    "            codes_ = json.loads(requests.get(url).text)\n",
    "            result2 = [code[\"code\"] for code in codes_]    \n",
    "            return {\"type\": \"region\", \"code\": result[0], \"codes_departements\": result2}\n",
    "        \n",
    "        elif len(similar_communes)==0 and len(similar_departements)==0:\n",
    "            result3 = {}\n",
    "            similar_regions = [code[\"code\"] for code in codes]\n",
    "            for similar_regions in similar_regions:\n",
    "                url = \"https://geo.api.gouv.fr/regions/{c}/departements\".format(c=similar_region)\n",
    "                codes_ = json.loads(requests.get(url).text)\n",
    "                result3[similar_region] = [code[\"code\"] for code in codes_]    \n",
    "                \n",
    "\n",
    "        \n",
    "    if len(similar_communes)>0:\n",
    "        return {\"type\": \"commune\", \"code\": similar_communes}\n",
    "    elif len(similar_departements)>0:\n",
    "        return {\"type\": \"departement\", \"code\": similar_departements}\n",
    "    elif len(similar_regions)>0:\n",
    "        return {\"type\": \"region\", \"code\": similar_regions, \"codes_departements\": result3}\n",
    "    else: \n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fWs4AmmWkdPO",
    "outputId": "7c7a0033-b914-49ab-8f82-d55deef73bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parisien\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_insee(\"parisien\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "AnJCYr5jmKQT"
   },
   "outputs": [],
   "source": [
    "def insee_to_bss(code_location, type_location):\n",
    "    \"\"\"\n",
    "    Returns the BSS codes corresponding to the INSEE code\n",
    "    \"\"\"\n",
    "    if type_location == 'commune':\n",
    "        url =  \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/stations?code_commune={c}&format=json&size=500\".format(c=code_location)\n",
    "   \n",
    "    elif type_location == 'departement':\n",
    "        url =  \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/stations?code_departement={c}&format=json&size=500\".format(c=code_location)\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    exists = json.loads(requests.get(url).text)[\"count\"]\n",
    "    if exists>0 :\n",
    "        data = json.loads(requests.get(url).text)\n",
    "        bss = [station[\"code_bss\"] for station in data[\"data\"]]\n",
    "        return bss\n",
    "\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "KVD99fybFgzV"
   },
   "outputs": [],
   "source": [
    "def get_mesure_piezo(station, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Returns mesures corresponding to the station BSS code\n",
    "    \"\"\"\n",
    "    url = \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/chroniques?code_bss={bss}&date_debut_mesure={d1}&date_fin_mesure={d2}&size=1\".format(bss=station, d1=start_date, d2=end_date)\n",
    "    number = json.loads(requests.get(url).text)[\"count\"]\n",
    "    if number > 0:\n",
    "        url = \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/chroniques?code_bss={bss}&date_debut_mesure={d1}&date_fin_mesure={d2}&size={s}\".format(bss=station, d1=start_date, d2=end_date, s=number + 1)\n",
    "        return json.loads(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_adj(text):\n",
    "    \"\"\"\n",
    "    Returns words that are tagged ADJ in query\n",
    "    uses stanfordNLP POS server on port 9000\n",
    "    \"\"\"\n",
    "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    splitted = text.split()\n",
    "    adjs = []\n",
    "    \n",
    "    for word in splitted:\n",
    "        result = nlp.annotate(word,\n",
    "                           properties={\n",
    "                               'annotators': 'pos',\n",
    "                               'outputFormat': 'json',\n",
    "                               'timeout': 1000,\n",
    "                           })\n",
    "        if  result[\"sentences\"][0][\"tokens\"][0][\"pos\"] == \"ADJ\":\n",
    "            adjs.append(result[\"sentences\"][0][\"tokens\"][0][\"word\"])\n",
    "    \n",
    "    return adjs\n",
    "\n",
    "def POS_adj2(text):\n",
    "    \"\"\"\n",
    "    Returns words that are tagged ADJ in query\n",
    "    uses stanfordNLP POS with the python wrapper stanza\n",
    "    \"\"\"\n",
    "#     stanza.download('fr')\n",
    "    nlp = stanza.Pipeline('fr', processors='tokenize, pos')\n",
    "    splitted = text.split()\n",
    "    adjs = []\n",
    "    result = nlp(text)\n",
    "    for sentence in result.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.pos==\"ADJ\":\n",
    "                adjs.append(word.text)\n",
    "    return adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-07 16:27:46 WARNING: Language fr package default expects mwt, which has been added\n",
      "2021-07-07 16:27:46 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-07-07 16:27:46 INFO: Use device: cpu\n",
      "2021-07-07 16:27:46 INFO: Loading: tokenize\n",
      "2021-07-07 16:27:46 INFO: Loading: mwt\n",
      "2021-07-07 16:27:46 INFO: Loading: pos\n",
      "2021-07-07 16:27:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['profonde', 'parisienne']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_adj2(\"A quelle profondeur se trouve la plus profonde nappe parisienne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\" stemming \"\"\"\n",
    "    word_ = \"\".join(list(word)[-4:])\n",
    "    return word[:-4]+re.sub(r'iens|ains|ards|ain|ien|ard|ois|oi|ens|en|ais|ai|ins|in|s$', '',word_, count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_from_adj(c, communes):   \n",
    "    \"\"\"\n",
    "    Returns the most similar commune to the adjective c from the list of communes\n",
    "    \"\"\"\n",
    "    c_ = stem(c)\n",
    "    dist = []\n",
    "    for a in communes :\n",
    "        limit = min(int(2*len(c)/3), len(a))\n",
    "        a1 =  \"\".join(list(a)[:limit])\n",
    "        c1 =  \"\".join(list(c)[:limit])\n",
    "        d1 = nltk.edit_distance(c1, a1)\n",
    "        d2 = nltk.edit_distance(c_, a)\n",
    "        dist.append([d1, d2])    \n",
    "    \n",
    "    dist = np.array(dist)\n",
    "    avg = 0.5*dist[:, 0] + 0.5*dist[:, 1]\n",
    "    sorted_ = np.argsort(avg)\n",
    "    \n",
    "    commune_ = communes[sorted_[0]]\n",
    "    return commune_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geolocation(ip_adress):\n",
    "    \"\"\" \n",
    "    Get location from ip adress\n",
    "    \"\"\"\n",
    "    response = DbIpCity.get(ip_address, api_key='free')\n",
    "    return response.city\n",
    "\n",
    "def get_geolocation_ipinfo(ip_adress):\n",
    "    \n",
    "    \"\"\" \n",
    "    Get location from ip adress with ipinfo API\n",
    "    \"\"\"\n",
    "    access_token = 'ea47e58acb96e4'\n",
    "    handler = ipinfo.getHandler(access_token)\n",
    "    details = handler.getDetails(ip_address)\n",
    "    city = details.city\n",
    "    return city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "C9qGKKe2RWT-"
   },
   "outputs": [],
   "source": [
    "def get_locations(query, ip_address=None):\n",
    "    \"\"\"\n",
    "    Use NER to extract locations from query,\n",
    "    if NER gives no result, look for demonyms and return corresponding location,\n",
    "    if none found, return geolocation\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = 4\n",
    "    MODEL_PATH = \"NER_tool/stacked-standard-flair-150-wikiner.pt\"\n",
    "    tag_type = \"label\"\n",
    "    model = SequenceTagger.load(MODEL_PATH)\n",
    "    \n",
    "    snippets = [[1, query]]\n",
    "    result = predict_flair.get_entities(snippets, model, tag_type, batch_size)[\"snippets\"][0][1]\n",
    "    locations = [entity[\"text\"] for entity in result[\"entities\"] if \"LOC\" in str(entity[\"labels\"][0])]\n",
    "    if len(locations)>0 :\n",
    "        return locations\n",
    "    \n",
    "    else : \n",
    "        adjs = POS_adj2(query)\n",
    "        locs = [get_location_from_adj(adj, communes) for adj in adjs] # get_location_from_adj will be modified\n",
    "                                                                      # to work with the dictionnary, for now it\n",
    "                                                                      # uses string similarity\n",
    "        if len(locs) > 0:\n",
    "            return locs        \n",
    "        else :\n",
    "            return get_geolocation(ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lon1, lat1, lon2, lat2):\n",
    "    \n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 12742 * asin(sqrt(a)) #2*R*asin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(bss):\n",
    "\n",
    "    url =  \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/stations?code_bss={c}&format=json&size=50\".format(c=bss)\n",
    "    exists = json.loads(requests.get(url).text)[\"count\"]\n",
    "    if exists>0 :\n",
    "        data = json.loads(requests.get(url).text)\n",
    "        info = data[\"data\"][0]\n",
    "        return {\"code_commune\":info[\"code_commune_insee\"], \"code_departement\":info[\"code_departement\"],\"long\": info[\"geometry\"][\"coordinates\"][0], \"lat\":info[\"geometry\"][\"coordinates\"][1], \"date_fin_mesure\":info[\"date_fin_mesure\"] }\n",
    "    else:\n",
    "        return -1 # bss code does not correspond to any station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_stations(bss, N=4): \n",
    "    info = get_coordinates(bss)\n",
    "    if info!=-1:\n",
    "        dep, long, lat = info[\"code_departement\"], info[\"long\"], info[\"lat\"]\n",
    "        dep_stations = insee_to_bss(dep, \"departement\")\n",
    "        dist = {}\n",
    "\n",
    "        for station in dep_stations:\n",
    "            _ = get_coordinates(station)\n",
    "            long_, lat_, date = _[\"long\"], _[\"lat\"] , _[\"date_fin_mesure\"]\n",
    "            if date is not None:\n",
    "                date = date.split(\"-\")\n",
    "                last_mesure_date = datetime(int(date[0]), int(date[1]), int(date[2]))\n",
    "\n",
    "                if last_mesure_date >= datetime(2005, 1, 1): # the last mesure date must be later than 01-01-2005\n",
    "                    dist[station] = distance(long, lat, long_, lat_)\n",
    "\n",
    "        sortd = dict(sorted(dist.items(), key=lambda item: item[1]))\n",
    "        return list(sortd.keys())[: min(N, len(sortd.keys()))]\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# relativedelta(today, last_mesure_date).years <= 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aast', 'Abainville', 'Abancourt', ..., 'Œting', 'Œuf-en-Ternois',\n",
       "       'Œuilly'], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communes = pd.read_csv(\"demonyms/Data/commune2021.csv\")[\"LIBELLE\"].to_numpy().reshape(-1)\n",
    "communes = np.unique(communes)\n",
    "communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03635X0545/PZ1', '03635X0545/F1']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bss(\"Quel était le niveau de la nappe phréatique à la station piézométrique 03635X0545/PZ1 03635X0545/F1 le 12 janvier 2019 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfMdQJH3Cq9K",
    "outputId": "c6bb3e7d-5590-499e-8c03-68480ed335e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03634X0049/P1']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bss  = insee_to_bss(\"45188\", \"commune\")\n",
    "bss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThlIijwxP7jb",
    "outputId": "52a3d97d-88d0-43d9-93a7-f200d19e0a88"
   },
   "outputs": [],
   "source": [
    "mesure = get_mesure_piezo(\"03634X0049/P1\", start_date = \"2019-01-12\", end_date = \"2019-01-30\")\n",
    "mesure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 14:52:09,605 loading file NER_tool/stacked-standard-flair-150-wikiner.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Orléans', 'Paris']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = get_locations(\"Quel est le niveau de la nappe phréatique à Orléans et Paris aujourd'hui?\")\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 14:52:13,266 loading file NER_tool/stacked-standard-flair-150-wikiner.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Coulmiers', 'Orléans']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = get_locations(\"A quelle profondeur se trouve la nappe à l'adresse 12 rue de Coulmiers, 45000 Orléans\")\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 14:52:46,557 loading file NER_tool/stacked-standard-flair-150-wikiner.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lyon']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = get_locations(\"Y'a-t-il de l'eau dans le sous-sol lyonnais\")\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "2021-07-01 14:56:01,557 loading file NER_tool/stacked-standard-flair-150-wikiner.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with urllib.request.urlopen(\"https://geolocation-db.com/json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    ip_address = data[\"IPv4\"]\n",
    "    \n",
    "locations = get_locations(\"Y'a-t-il de l'eau dans le sous-sol\", ip_address=ip_address)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 19:28:12,353 loading file NER_tool/stacked-standard-flair-150-wikiner.pt\n",
      "['profonde', 'parisienne']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['piffonds', 'paris']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = get_locations(\"A quelle profondeur se trouve la plus profonde nappe parisienne \")\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_geolocation_ipinfo(ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03634X0093/F', '03633X0081/P', '03278X0020/P', '03637X0122/P']"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_stations('03634X0049/P1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hub'eau_trial1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
